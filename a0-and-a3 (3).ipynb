{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8fb958",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-03T11:54:21.625797Z",
     "iopub.status.busy": "2025-03-03T11:54:21.625446Z",
     "iopub.status.idle": "2025-03-03T11:54:27.682482Z",
     "shell.execute_reply": "2025-03-03T11:54:27.681544Z"
    },
    "papermill": {
     "duration": 6.064094,
     "end_time": "2025-03-03T11:54:27.684093",
     "exception": false,
     "start_time": "2025-03-03T11:54:21.619999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        pass\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb1bfc60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:54:27.693106Z",
     "iopub.status.busy": "2025-03-03T11:54:27.692732Z",
     "iopub.status.idle": "2025-03-03T11:56:29.451264Z",
     "shell.execute_reply": "2025-03-03T11:56:29.450375Z"
    },
    "papermill": {
     "duration": 121.764597,
     "end_time": "2025-03-03T11:56:29.453014",
     "exception": false,
     "start_time": "2025-03-03T11:54:27.688417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mCollecting nvidia-cudnn-cu11\r\n",
      "  Downloading nvidia_cudnn_cu11-9.7.1.26-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cublas-cu11 (from nvidia-cudnn-cu11)\r\n",
      "  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Downloading nvidia_cudnn_cu11-9.7.1.26-py3-none-manylinux_2_27_x86_64.whl (520.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.9/520.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl (417.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11\r\n",
      "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cudnn-cu11-9.7.1.26\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q \"tf-models-official\"\n",
    "!pip install -U nvidia-cudnn-cu11\n",
    "\n",
    "# Install the mediapy package for visualizing images/videos.\n",
    "# See https://github.com/google/mediapy\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "!pip install -q mediapy remotezip\n",
    "!pip install -U -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe075b94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:56:29.502516Z",
     "iopub.status.busy": "2025-03-03T11:56:29.502226Z",
     "iopub.status.idle": "2025-03-03T11:56:48.742610Z",
     "shell.execute_reply": "2025-03-03T11:56:48.741751Z"
    },
    "papermill": {
     "duration": 19.266562,
     "end_time": "2025-03-03T11:56:48.744256",
     "exception": false,
     "start_time": "2025-03-03T11:56:29.477694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install tf-models-official  --quiet\n",
    "!pip install tensorflow-docs --quiet\n",
    "!pip install -U kaleido --quiet\n",
    "!pip install tf-keras-vis --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d2de18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:56:48.794819Z",
     "iopub.status.busy": "2025-03-03T11:56:48.794451Z",
     "iopub.status.idle": "2025-03-03T11:56:56.268594Z",
     "shell.execute_reply": "2025-03-03T11:56:56.267894Z"
    },
    "papermill": {
     "duration": 7.500881,
     "end_time": "2025-03-03T11:56:56.270133",
     "exception": false,
     "start_time": "2025-03-03T11:56:48.769252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d521d95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:56:56.320840Z",
     "iopub.status.busy": "2025-03-03T11:56:56.320259Z",
     "iopub.status.idle": "2025-03-03T11:56:57.561595Z",
     "shell.execute_reply": "2025-03-03T11:56:57.560921Z"
    },
    "papermill": {
     "duration": 1.268005,
     "end_time": "2025-03-03T11:56:57.563143",
     "exception": false,
     "start_time": "2025-03-03T11:56:56.295138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from official.projects.movinet.modeling import movinet\n",
    "from official.projects.movinet.modeling import movinet_model\n",
    "from official.projects.movinet.tools import export_saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e8356ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:56:57.613402Z",
     "iopub.status.busy": "2025-03-03T11:56:57.613145Z",
     "iopub.status.idle": "2025-03-03T11:57:20.808664Z",
     "shell.execute_reply": "2025-03-03T11:57:20.807718Z"
    },
    "papermill": {
     "duration": 23.22202,
     "end_time": "2025-03-03T11:57:20.810141",
     "exception": false,
     "start_time": "2025-03-03T11:56:57.588121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 8\n",
      "Num videos for class shortpass: 354\n",
      "Num videos for class longpass: 147\n",
      "Num videos for class throw: 367\n",
      "Num videos for class goalkick: 425\n",
      "Num videos for class penalty: 322\n",
      "Num videos for class corner: 411\n",
      "Num videos for class freekick: 374\n",
      "Num videos for class ontarget: 340\n",
      "\n",
      "Average frames and length per class:\n",
      "shortpass:\n",
      "  Avg frames: 75.86\n",
      "  Avg length: 2.99 seconds\n",
      "longpass:\n",
      "  Avg frames: 101.80\n",
      "  Avg length: 4.07 seconds\n",
      "throw:\n",
      "  Avg frames: 62.64\n",
      "  Avg length: 2.46 seconds\n",
      "goalkick:\n",
      "  Avg frames: 73.02\n",
      "  Avg length: 2.88 seconds\n",
      "penalty:\n",
      "  Avg frames: 71.91\n",
      "  Avg length: 2.86 seconds\n",
      "corner:\n",
      "  Avg frames: 75.38\n",
      "  Avg length: 2.89 seconds\n",
      "freekick:\n",
      "  Avg frames: 74.21\n",
      "  Avg length: 2.88 seconds\n",
      "ontarget:\n",
      "  Avg frames: 69.32\n",
      "  Avg length: 2.48 seconds\n"
     ]
    }
   ],
   "source": [
    "def classes_available(path):\n",
    "    return [c for c in os.listdir(path)]\n",
    "\n",
    "def files_per_class(path):\n",
    "    class_instances = dict()\n",
    "    for c in classes_available(path):\n",
    "        class_path = os.path.join(path,c)\n",
    "        files = list()\n",
    "        for file in os.listdir(class_path):\n",
    "            files.append(os.path.join(class_path,file))\n",
    "        class_instances[c.split('-')[0]] = files\n",
    "\n",
    "    return class_instances\n",
    "\n",
    "t = files_per_class('/kaggle/input/footballactions/FootballActions')\n",
    "\n",
    "print('Num classes:', len(list(t.keys())))\n",
    "for i in t.keys():\n",
    "    print(f'Num videos for class {i}:' ,len(t[i]))\n",
    "\n",
    "\n",
    "def average_frames_and_length_per_class(path):\n",
    "    class_instances = files_per_class(path)\n",
    "    avg_frames_per_class = {}\n",
    "    avg_length_per_class = {}\n",
    "\n",
    "    for class_name, files in class_instances.items():\n",
    "        total_frames = 0\n",
    "        total_length = 0\n",
    "        for file in files:\n",
    "            cap = cv2.VideoCapture(file)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            duration = frame_count / fps if fps > 0 else 0\n",
    "\n",
    "            total_frames += frame_count\n",
    "            total_length += duration\n",
    "            cap.release()\n",
    "\n",
    "        num_files = len(files)\n",
    "        avg_frames = total_frames / num_files if num_files else 0\n",
    "        avg_length = total_length / num_files if num_files else 0\n",
    "\n",
    "        avg_frames_per_class[class_name] = avg_frames\n",
    "        avg_length_per_class[class_name] = avg_length\n",
    "\n",
    "    return avg_frames_per_class, avg_length_per_class\n",
    "\n",
    "\n",
    "avg_frames_per_class, avg_length_per_class = average_frames_and_length_per_class('/kaggle/input/footballactions/FootballActions')\n",
    "\n",
    "print('\\nAverage frames and length per class:')\n",
    "for class_name in avg_frames_per_class.keys():\n",
    "    print(f'{class_name}:')\n",
    "    print(f'  Avg frames: {avg_frames_per_class[class_name]:.2f}')\n",
    "    print(f'  Avg length: {avg_length_per_class[class_name]:.2f} seconds')\n",
    "\n",
    "def train_test_split(dataset_dict, test_split=0.2, seed=42):\n",
    "    random.seed(seed)\n",
    "    train_dict = dict()\n",
    "    test_dict = dict()\n",
    "\n",
    "    for class_name, files in dataset_dict.items():\n",
    "        random.shuffle(files)\n",
    "        split_index = int(len(files) * (1 - test_split))\n",
    "        train_files = files[:split_index]\n",
    "        test_files = files[split_index:]\n",
    "        train_dict[class_name] = train_files\n",
    "        test_dict[class_name] = test_files\n",
    "\n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8593c86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:20.861136Z",
     "iopub.status.busy": "2025-03-03T11:57:20.860859Z",
     "iopub.status.idle": "2025-03-03T11:57:22.416343Z",
     "shell.execute_reply": "2025-03-03T11:57:22.415639Z"
    },
    "papermill": {
     "duration": 1.582218,
     "end_time": "2025-03-03T11:57:22.418005",
     "exception": false,
     "start_time": "2025-03-03T11:57:20.835787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b7f5e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:22.470342Z",
     "iopub.status.busy": "2025-03-03T11:57:22.469482Z",
     "iopub.status.idle": "2025-03-03T11:57:22.481973Z",
     "shell.execute_reply": "2025-03-03T11:57:22.481205Z"
    },
    "papermill": {
     "duration": 0.039501,
     "end_time": "2025-03-03T11:57:22.483284",
     "exception": false,
     "start_time": "2025-03-03T11:57:22.443783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def per_model(t, train_dict, test_dict, model_class, split):\n",
    "    split_percentage = 100 * split\n",
    "    classes = list(t.keys())\n",
    "\n",
    "    # Get train and test counts per class\n",
    "    train_counts = [len(train_dict[c]) for c in classes]\n",
    "    test_counts = [len(test_dict[c]) for c in classes]\n",
    "\n",
    "    # Sort classes by total count (train + test)\n",
    "    sorted_data = sorted(zip(classes, train_counts, test_counts), key=lambda x: x[1] + x[2], reverse=True)\n",
    "    sorted_classes, sorted_train_counts, sorted_test_counts = zip(*sorted_data)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add train data bars with labels\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=sorted_classes,\n",
    "        y=sorted_train_counts,\n",
    "        name=\"Train\",\n",
    "        marker_color=\"#9370DB\",  # Medium purple\n",
    "        text=[f\"{count:,}\" for count in sorted_train_counts],\n",
    "        textposition=\"outside\"  # Moves labels outside bars\n",
    "    ))\n",
    "\n",
    "    # Add test data bars with labels\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=sorted_classes,\n",
    "        y=sorted_test_counts,\n",
    "        name=\"Test\",\n",
    "        marker_color=\"#D3D3D3\",  # Light gray\n",
    "        text=[f\"{count:,}\" for count in sorted_test_counts],\n",
    "        textposition=\"outside\"\n",
    "    ))\n",
    "\n",
    "    # Calculate overall dataset statistics\n",
    "    total_train = sum(train_counts)\n",
    "    total_test = sum(test_counts)\n",
    "    total_samples = total_train + total_test\n",
    "\n",
    "    # Add total annotations above bars\n",
    "    for i, (cls, train, test) in enumerate(zip(sorted_classes, sorted_train_counts, sorted_test_counts)):\n",
    "        total_count = train + test\n",
    "        percentage = (total_count / total_samples) * 100\n",
    "        fig.add_annotation(\n",
    "            x=cls, y=total_count + max(total_samples * 0.02, 5),  # Offset for visibility\n",
    "            text=f\"{total_count:,} ({percentage:.1f}%)\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=12, color=\"black\")\n",
    "        )\n",
    "\n",
    "    # Layout updates\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"Train-Test Split Distribution for {model_class}\",\n",
    "            font=dict(family=\"Segoe Print\", size=24),\n",
    "            y=0.95, x=0.5, xanchor=\"center\", yanchor=\"top\"\n",
    "        ),\n",
    "        xaxis_title=dict(text=\"Class\", font=dict(family=\"Segoe Print\", size=18)),\n",
    "        yaxis_title=dict(text=\"Number of Videos\", font=dict(family=\"Segoe Print\", size=18)),\n",
    "        annotations=[\n",
    "            dict(\n",
    "                x=0.5, y=1.1,\n",
    "                text=f\"Total: {total_samples:,} | Train: {total_train:,} ({total_train / total_samples:.1%}) | Test: {total_test:,} ({total_test / total_samples:.1%})\",\n",
    "                xref=\"paper\", yref=\"paper\",\n",
    "                showarrow=False,\n",
    "                font=dict(family=\"Segoe Print\", size=16, color=\"darkred\")\n",
    "            )\n",
    "        ],\n",
    "        hovermode=False,  # No need for hover since it's a static image\n",
    "        margin=dict(t=150),\n",
    "        font=dict(family=\"Segoe Print\"),\n",
    "        plot_bgcolor=\"#F8F8F8\",\n",
    "        paper_bgcolor=\"#F8F8F8\"\n",
    "    )\n",
    "\n",
    "    # X-axis formatting\n",
    "    fig.update_xaxes(\n",
    "        tickangle=45,\n",
    "        tickfont=dict(family=\"Segoe Print\", size=12),\n",
    "        showline=True,\n",
    "        linewidth=1,\n",
    "        linecolor=\"black\"\n",
    "    )\n",
    "\n",
    "    # Y-axis formatting\n",
    "    fig.update_yaxes(\n",
    "        gridcolor=\"white\",\n",
    "        griddash=\"dash\"\n",
    "    )\n",
    "\n",
    "    # Save as PNG\n",
    "    file_name = f\"train_test_split_{model_class}_split_{100 - split_percentage:.0f}_{split_percentage:.0f}.png\"\n",
    "    fig.write_image(file_name, scale=3, width=1200, height=800)\n",
    "    print(f\"Plot saved: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892bd124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:22.534471Z",
     "iopub.status.busy": "2025-03-03T11:57:22.534180Z",
     "iopub.status.idle": "2025-03-03T11:57:22.538403Z",
     "shell.execute_reply": "2025-03-03T11:57:22.537739Z"
    },
    "papermill": {
     "duration": 0.031196,
     "end_time": "2025-03-03T11:57:22.539705",
     "exception": false,
     "start_time": "2025-03-03T11:57:22.508509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    return frame\n",
    "    \n",
    "def create_blank_frames(n_frames, output_size):\n",
    "    \"\"\"\n",
    "    Creates blank frames for error cases.\n",
    "    \"\"\"\n",
    "    return np.zeros((n_frames, output_size[0], output_size[1], 3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af96b38f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:22.590917Z",
     "iopub.status.busy": "2025-03-03T11:57:22.590588Z",
     "iopub.status.idle": "2025-03-03T11:57:22.596258Z",
     "shell.execute_reply": "2025-03-03T11:57:22.595637Z"
    },
    "papermill": {
     "duration": 0.032445,
     "end_time": "2025-03-03T11:57:22.597377",
     "exception": false,
     "start_time": "2025-03-03T11:57:22.564932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, n_frames, output_size = (172,172), frame_step = 15):\n",
    "      \"\"\"\n",
    "        Creates frames from each video file present for each category.\n",
    "    \n",
    "        Args:\n",
    "          video_path: File path to the video.\n",
    "          n_frames: Number of frames to be created per video file.\n",
    "          output_size: Pixel size of the output frame image.\n",
    "    \n",
    "        Return:\n",
    "          An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "      \"\"\"\n",
    "      # Read each video frame by frame\n",
    "      result = []\n",
    "      src = cv2.VideoCapture(str(video_path))\n",
    "    \n",
    "      video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    \n",
    "      need_length = 1 + (n_frames - 1) * frame_step\n",
    "    \n",
    "      if need_length > video_length:\n",
    "        start = 0\n",
    "      else:\n",
    "        max_start = video_length - need_length\n",
    "        start = random.randint(0, max_start + 1)\n",
    "    \n",
    "      src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "      # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "      ret, frame = src.read()\n",
    "      result.append(format_frames(frame, output_size))  \n",
    "      for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "          ret, frame = src.read()\n",
    "        if ret:\n",
    "          frame = format_frames(frame, output_size)\n",
    "          result.append(frame)\n",
    "        else:\n",
    "          result.append(np.zeros_like(result[0]))\n",
    "      src.release()\n",
    "      result = np.array(result)[..., [2, 1, 0]]\n",
    "    \n",
    "      return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "522eb5ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:22.647262Z",
     "iopub.status.busy": "2025-03-03T11:57:22.647009Z",
     "iopub.status.idle": "2025-03-03T11:57:22.652626Z",
     "shell.execute_reply": "2025-03-03T11:57:22.652011Z"
    },
    "papermill": {
     "duration": 0.032094,
     "end_time": "2025-03-03T11:57:22.653974",
     "exception": false,
     "start_time": "2025-03-03T11:57:22.621880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class FrameGenerator:\n",
    "    def __init__(self, dataset_dict, num_frames, size=290, training=False):\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.num_frames = num_frames\n",
    "        self.size = size\n",
    "        self.training = training\n",
    "        self.class_names = sorted(dataset_dict.keys())\n",
    "        self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "    \n",
    "    def __call__(self):\n",
    "        video_paths = []\n",
    "        classes = []\n",
    "        for class_name, files in self.dataset_dict.items():\n",
    "            video_paths.extend(files)\n",
    "            classes.extend([class_name] * len(files))\n",
    "        \n",
    "        pairs = list(zip(video_paths, classes))\n",
    "        if self.training:\n",
    "            random.shuffle(pairs)\n",
    "            \n",
    "        for video_path, class_name in pairs:\n",
    "            try:\n",
    "                video_frames = frames_from_video_file(video_path, \n",
    "                                                   self.num_frames, \n",
    "                                                   (self.size, self.size))\n",
    "                label = self.class_ids_for_name[class_name]\n",
    "                yield video_frames, label\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing video {video_path}: {str(e)}\")\n",
    "                yield create_blank_frames(self.num_frames, (self.size, self.size)), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a9819f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:22.705178Z",
     "iopub.status.busy": "2025-03-03T11:57:22.704876Z",
     "iopub.status.idle": "2025-03-03T11:57:22.708460Z",
     "shell.execute_reply": "2025-03-03T11:57:22.707642Z"
    },
    "papermill": {
     "duration": 0.030713,
     "end_time": "2025-03-03T11:57:22.709635",
     "exception": false,
     "start_time": "2025-03-03T11:57:22.678922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "        # 'A0': {'batch_size': 10, 'num_frames': 8, 'resolution': 172,'split':0.20},\n",
    "        'A3': {'batch_size': 8, 'num_frames': 8, 'resolution': 256,'split':0.30},     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "903b53c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:22.764747Z",
     "iopub.status.busy": "2025-03-03T11:57:22.764337Z",
     "iopub.status.idle": "2025-03-03T11:57:22.769159Z",
     "shell.execute_reply": "2025-03-03T11:57:22.768256Z"
    },
    "papermill": {
     "duration": 0.035315,
     "end_time": "2025-03-03T11:57:22.770486",
     "exception": false,
     "start_time": "2025-03-03T11:57:22.735171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(generator,batch_size):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int16)\n",
    "        )\n",
    "    )\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0def49df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:22.821437Z",
     "iopub.status.busy": "2025-03-03T11:57:22.821157Z",
     "iopub.status.idle": "2025-03-03T11:57:22.825163Z",
     "shell.execute_reply": "2025-03-03T11:57:22.824328Z"
    },
    "papermill": {
     "duration": 0.031122,
     "end_time": "2025-03-03T11:57:22.826516",
     "exception": false,
     "start_time": "2025-03-03T11:57:22.795394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
    "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
    "  model = movinet_model.MovinetClassifier(\n",
    "      backbone=backbone,\n",
    "      num_classes=num_classes)\n",
    "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6297ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:22.884752Z",
     "iopub.status.busy": "2025-03-03T11:57:22.884457Z",
     "iopub.status.idle": "2025-03-03T11:57:22.892388Z",
     "shell.execute_reply": "2025-03-03T11:57:22.891733Z"
    },
    "papermill": {
     "duration": 0.042106,
     "end_time": "2025-03-03T11:57:22.893583",
     "exception": false,
     "start_time": "2025-03-03T11:57:22.851477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomLearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'value'):\n",
    "            lr = lr.value()\n",
    "        tf.summary.scalar('learning_rate', data=lr, step=epoch)\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        logs['lr'] = lr\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = logs.get('lr', None)\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False\n",
    "\n",
    "class CustomReduceLROnPlateau(ReduceLROnPlateau):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = self.model.optimizer.lr.numpy()\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "class CustomEarlyStopping(EarlyStopping):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False\n",
    "\n",
    "class CustomLearningRateScheduler(LearningRateScheduler):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False\n",
    "\n",
    "class CustomTensorBoard(TensorBoard):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba201e03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:22.945491Z",
     "iopub.status.busy": "2025-03-03T11:57:22.945209Z",
     "iopub.status.idle": "2025-03-03T11:57:22.948838Z",
     "shell.execute_reply": "2025-03-03T11:57:22.948157Z"
    },
    "papermill": {
     "duration": 0.031092,
     "end_time": "2025-03-03T11:57:22.950108",
     "exception": false,
     "start_time": "2025-03-03T11:57:22.919016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de0aae0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:23.001544Z",
     "iopub.status.busy": "2025-03-03T11:57:23.001249Z",
     "iopub.status.idle": "2025-03-03T11:57:23.195885Z",
     "shell.execute_reply": "2025-03-03T11:57:23.195176Z"
    },
    "papermill": {
     "duration": 0.221864,
     "end_time": "2025-03-03T11:57:23.197365",
     "exception": false,
     "start_time": "2025-03-03T11:57:22.975501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def get_actual_predicted_labels(dataset, model):\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    for images, labels in dataset:\n",
    "        actual.extend(labels.numpy())\n",
    "        preds = model.predict(images)\n",
    "        predicted.extend(tf.argmax(preds, axis=1).numpy())\n",
    "    return np.array(actual), np.array(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59764cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:23.295989Z",
     "iopub.status.busy": "2025-03-03T11:57:23.295622Z",
     "iopub.status.idle": "2025-03-03T11:57:23.307139Z",
     "shell.execute_reply": "2025-03-03T11:57:23.306455Z"
    },
    "papermill": {
     "duration": 0.084881,
     "end_time": "2025-03-03T11:57:23.308328",
     "exception": false,
     "start_time": "2025-03-03T11:57:23.223447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def plot_confusion_matrix(actual, predicted, labels, ds_type, model_class, accuracy, split, normalize=False):\n",
    "    split = split * 100\n",
    "    \n",
    "    def create_cm_plot(ax, cm, cmap, title):\n",
    "        sns.heatmap(cm, annot=True, fmt=fmt, cmap=cmap,\n",
    "                    xticklabels=labels, yticklabels=labels,\n",
    "                    annot_kws={\"size\": 10, \"weight\": \"bold\"}, square=True, \n",
    "                    cbar_kws={'shrink': .8}, ax=ax, linewidths=0.5, linecolor='gray')\n",
    "        \n",
    "        ax.set_title(title, fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Predicted Action', fontsize=14, fontweight='bold', labelpad=15)\n",
    "        ax.set_ylabel('Actual Action', fontsize=14, fontweight='bold', labelpad=15)\n",
    "        \n",
    "        # Improve tick label visibility\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\", fontsize=10, fontweight='bold')\n",
    "        plt.setp(ax.get_yticklabels(), rotation=0, ha=\"right\", fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Add value annotations\n",
    "        thresh = cm.max() / 2\n",
    "        for i, j in np.ndindex(cm.shape):\n",
    "            ax.text(j + 0.5, i + 0.5, f\"{cm[i, j]:.0f}\" if not normalize else f\"{cm[i, j]:.2f}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                    fontweight='bold', fontsize=10)\n",
    "\n",
    "        # Add borders to the heatmap\n",
    "        for _, spine in ax.spines.items():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_linewidth(2)\n",
    "            spine.set_color('black')\n",
    "\n",
    "    try:\n",
    "        actual = np.ravel(actual)\n",
    "        predicted = np.ravel(predicted)\n",
    "        cm = confusion_matrix(actual, predicted)\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            fmt = '.2f'\n",
    "        else:\n",
    "            fmt = 'd'\n",
    "        \n",
    "        fig, (ax_cm, ax_guide) = plt.subplots(1, 2, figsize=(24, 12), \n",
    "                                              gridspec_kw={'width_ratios': [1.5, 1]})\n",
    "        \n",
    "        # Custom colormap for better visibility\n",
    "        cmap = sns.color_palette(\"Blues\", as_cmap=True)\n",
    "        create_cm_plot(ax_cm, cm, cmap, 'Confusion Matrix')\n",
    "        \n",
    "        # Add interpretation guide\n",
    "        ax_guide.axis('off')\n",
    "        guide_text = \"\"\"\n",
    "        Interpretation Guide:\n",
    "        \n",
    "        1. Diagonal Elements: \n",
    "           Represent correct predictions. Higher values are better.\n",
    "        \n",
    "        2. Off-Diagonal Elements: \n",
    "           Show misclassifications. Lower values are better.\n",
    "        \n",
    "        3. Row Sums: \n",
    "           Total actual instances for each class.\n",
    "        \n",
    "        4. Column Sums: \n",
    "           Total predicted instances for each class.\n",
    "        \n",
    "        5. Color Intensity: \n",
    "           Darker blues indicate higher values.\n",
    "        \n",
    "        6. Normalized vs. Raw: \n",
    "           - Raw: Shows actual counts\n",
    "           - Normalized: Shows proportions (sum to 1 for each row)\n",
    "        \n",
    "        7. Common Patterns:\n",
    "           - Strong Diagonal: Good overall performance\n",
    "           - Weak Diagonal: Poor overall performance\n",
    "           - Dark Off-Diagonal: Common misclassification\n",
    "        \n",
    "        8. Class Imbalance: \n",
    "           Look for significantly different row sums.\n",
    "        \n",
    "        9. Precision vs. Recall:\n",
    "           - Precision: Look down columns (few false positives)\n",
    "           - Recall: Look across rows (few false negatives)\n",
    "        \n",
    "        10. Overall Accuracy:\n",
    "            Sum of diagonal elements divided by total predictions.\n",
    "        \"\"\"\n",
    "        ax_guide.text(0, 1, guide_text, va='top', ha='left', fontsize=12, \n",
    "                      bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.3),\n",
    "                      fontweight='medium')\n",
    "        \n",
    "        plt.suptitle(f'Confusion Matrix: MoViNet-{model_class}-Stream ({ds_type})\\n'\n",
    "                     f'Accuracy: {accuracy*100:.4f}%', fontsize=22, fontweight='bold', y=1.02)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fname = f'confusion_matrix_{model_class}_{ds_type}_accuracy_{accuracy*100:.4f}.png'\n",
    "        plt.savefig(fname, bbox_inches='tight', dpi=600)\n",
    "        plt.close()\n",
    "        print(f\"Enhanced confusion matrix saved as {fname}\")\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(actual, predicted, target_names=labels, output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_fname = f'classification_report_{model_class}_{ds_type}_at_training_split_{100-split}_{split}.csv'\n",
    "        report_df.to_csv(report_fname)\n",
    "        print(f\"Classification report saved as {report_fname}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plotting confusion matrix: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da7c8f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:23.359877Z",
     "iopub.status.busy": "2025-03-03T11:57:23.359545Z",
     "iopub.status.idle": "2025-03-03T11:57:23.364874Z",
     "shell.execute_reply": "2025-03-03T11:57:23.364056Z"
    },
    "papermill": {
     "duration": 0.032533,
     "end_time": "2025-03-03T11:57:23.366226",
     "exception": false,
     "start_time": "2025-03-03T11:57:23.333693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix2(actual, predicted, labels, ds_type,model_class,accuracy,split):\n",
    "  cm = tf.math.confusion_matrix(actual, predicted)\n",
    "  split = 100 * split\n",
    "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "  sns.set(rc={'figure.figsize':(6, 16)})\n",
    "  sns.set(font_scale=1.4)\n",
    "  plt.title(f'Confusion Matrix: MoViNet-{model_class}-Stream ({ds_type})\\n'\n",
    "                  f'Accuracy: {accuracy*100:.4f}',\n",
    "                  fontsize=16, pad=16)\n",
    "  ax.set_xlabel('Predicted Action')\n",
    "  ax.set_ylabel('Actual Action')\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.yticks(rotation=0)\n",
    "  ax.xaxis.set_ticklabels(labels)\n",
    "  ax.yaxis.set_ticklabels(labels)\n",
    "  plt.tight_layout()\n",
    "  fname = f'confusion_matrix_alternative_presentation_{model_class}_{ds_type}_accuracy_{accuracy*100:.4f}%_at_training_split_{100-split}_{split}.png'\n",
    "  plt.savefig(fname, bbox_inches='tight', dpi=300)\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7ffe47a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:23.417238Z",
     "iopub.status.busy": "2025-03-03T11:57:23.416975Z",
     "iopub.status.idle": "2025-03-03T11:57:23.430934Z",
     "shell.execute_reply": "2025-03-03T11:57:23.430224Z"
    },
    "papermill": {
     "duration": 0.040948,
     "end_time": "2025-03-03T11:57:23.432146",
     "exception": false,
     "start_time": "2025-03-03T11:57:23.391198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def plot_training_metrics(results, model_id, initial_lr, batch_size, optimizer, resolution, split, accuracy):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics (accuracy, loss, and learning rate) over epochs with improved visuals for print.\n",
    "    \n",
    "    Args:\n",
    "    results (keras.callbacks.History): History object returned by model.fit().\n",
    "    model_id (str): Identifier for the model.\n",
    "    initial_lr (float): Initial learning rate.\n",
    "    batch_size (int): Batch size used for training.\n",
    "    optimizer (str): Name of the optimizer used.\n",
    "    resolution (int): Input resolution for the model.\n",
    "    split (float): Train-validation split ratio.\n",
    "    accuracy (float): Test accuracy of the model.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    history = results.history  # Extract the history dictionary from the History object\n",
    "    split = 100 * split\n",
    "    plt.style.use('seaborn')\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "    gs = GridSpec(4, 2, figure=fig, height_ratios=[1, 1, 1, 0.5])\n",
    "    \n",
    "    # Colors\n",
    "    colors = {'train': '#C71585',  # Deep pink\n",
    "              'val': '#696969',    # Dim gray\n",
    "              'lr': '#FFA500'}     # Orange\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = ['accuracy', 'loss']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = fig.add_subplot(gs[i, :])\n",
    "        epochs = range(1, len(history[metric]) + 1)\n",
    "        \n",
    "        # Training data\n",
    "        ax.plot(epochs, history[metric], color=colors['train'], label=f'Training {metric.capitalize()}', linewidth=2, marker='o')\n",
    "        \n",
    "        # Validation data\n",
    "        ax.plot(epochs, history[f'val_{metric}'], color=colors['val'], label=f'Validation {metric.capitalize()}', linewidth=2, marker='o')\n",
    "        \n",
    "        # Add annotations for best/worst points\n",
    "        best_epoch = np.argmax(history[f'val_{metric}']) if metric == 'accuracy' else np.argmin(history[f'val_{metric}'])\n",
    "        best_value = history[f'val_{metric}'][best_epoch]\n",
    "        ax.annotate(f'Best: {best_value:.4f}', xy=(best_epoch+1, best_value), xytext=(5, 5), \n",
    "                    textcoords='offset points', ha='left', va='bottom',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "        \n",
    "        ax.set_title(f'{metric.capitalize()} Over Epochs', fontsize=16)\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel(metric.capitalize(), fontsize=12)\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Learning Rate Over Epochs\n",
    "    ax = fig.add_subplot(gs[2, :])\n",
    "    if 'lr' in history:\n",
    "        ax.plot(epochs, history['lr'], color=colors['lr'], linewidth=2, marker='o')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_title('Learning Rate Over Epochs', fontsize=16)\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel('Learning Rate', fontsize=12)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Annotate initial and final learning rates\n",
    "        ax.annotate(f'Initial LR: {history[\"lr\"][0]:.2e}', xy=(1, history[\"lr\"][0]), xytext=(5, 5), \n",
    "                    textcoords='offset points', ha='left', va='bottom',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "        ax.annotate(f'Final LR: {history[\"lr\"][-1]:.2e}', xy=(len(epochs), history[\"lr\"][-1]), xytext=(5, 5), \n",
    "                    textcoords='offset points', ha='right', va='top',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Learning Rate data not available', ha='center', va='center', fontsize=16)\n",
    "    \n",
    "    # Info text\n",
    "    info_text = [f\"Model: {model_id}\",\n",
    "                 f\"Initial Learning Rate: {initial_lr}\",\n",
    "                 f\"Final Learning Rate: {history['lr'][-1]:.2e}\" if 'lr' in history else \"Final Learning Rate: Not available\",\n",
    "                 f\"Batch Size: {batch_size}\",\n",
    "                 f\"Optimizer: {optimizer}\",\n",
    "                 f\"Resolution: {resolution}\",\n",
    "                 f\"Total Epochs: {len(history['accuracy'])}\",\n",
    "                 f\"Early Stopping: {'Yes' if len(history['accuracy']) < 30 else 'No'}\",\n",
    "                 f\"Train-Val Split: {100 - split:.2f}/{split:.2f}\"]\n",
    "\n",
    "    # Summary text\n",
    "    summary_text = [f\"Test Accuracy: {accuracy*100:.2f}%\",\n",
    "                    f\"Best Val Accuracy: {max(history['val_accuracy'])*100:.2f}% (Epoch {np.argmax(history['val_accuracy'])+1})\",\n",
    "                    f\"Lowest Val Loss: {min(history['val_loss']):.4f} (Epoch {np.argmin(history['val_loss'])+1})\",\n",
    "                    f\"Overfitting: {'Not observed' if max(history['accuracy']) - max(history['val_accuracy']) < 0.05 else 'Observed'}\"]\n",
    "\n",
    "    # Add info and summary as text\n",
    "    ax_info = fig.add_subplot(gs[3, 0])\n",
    "    ax_info.axis('off')\n",
    "    ax_info.text(0, 1, '\\n'.join(info_text), verticalalignment='top', fontsize=12)\n",
    "    ax_info.set_title('Model Information', fontsize=16)\n",
    "\n",
    "    ax_summary = fig.add_subplot(gs[3, 1])\n",
    "    ax_summary.axis('off')\n",
    "    ax_summary.text(0, 1, '\\n'.join(summary_text), verticalalignment='top', fontsize=12)\n",
    "    ax_summary.set_title('Performance Summary', fontsize=16)\n",
    "\n",
    "    # Overall title\n",
    "    fig.suptitle(f\"Training Metrics for MoviNet {model_id} Stream at {100 - split}%-{split}% train test split\", fontsize=20, y=0.95)\n",
    "\n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(f\"training_metrics_for_{model_id}_at_train-test_split_of_{split}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f7c08ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:23.485168Z",
     "iopub.status.busy": "2025-03-03T11:57:23.484859Z",
     "iopub.status.idle": "2025-03-03T11:57:23.493519Z",
     "shell.execute_reply": "2025-03-03T11:57:23.492695Z"
    },
    "papermill": {
     "duration": 0.037122,
     "end_time": "2025-03-03T11:57:23.494815",
     "exception": false,
     "start_time": "2025-03-03T11:57:23.457693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def export_model_variants(model, model_id, resolution, num_frames, accuracy, split, train_dict, model_weights):\n",
    "    \"\"\"\n",
    "    Export the model in various formats for single-batch and multi-batch scenarios,\n",
    "    as well as multi-frame scenarios.\n",
    "    \n",
    "    Args:\n",
    "    model: The TensorFlow model to export\n",
    "    model_id (str): Identifier for the model\n",
    "    resolution (int): Input resolution for the model\n",
    "    accuracy (float): Accuracy of the model\n",
    "    \"\"\"\n",
    "    exported_models = {}\n",
    "    split = 100 * split\n",
    "\n",
    "    # Calculate initial model size and parameters\n",
    "    initial_model_size = get_model_size(model_weights)\n",
    "    initial_params = model.count_params()\n",
    "\n",
    "    # Determine initial numerical operations\n",
    "    initial_num_ops = get_numerical_operations(model)\n",
    "\n",
    "    for batch_size, batch_type in zip([1, 2], ['single', 'dual']):\n",
    "        saved_model_dir = f'saved_model_{batch_type}_batch_{num_frames}_frames'\n",
    "        \n",
    "        input_shape = [batch_size, num_frames, resolution, resolution, 3]\n",
    "        \n",
    "        export_saved_model.export_saved_model(\n",
    "            model=model,\n",
    "            input_shape=input_shape,\n",
    "            export_path=saved_model_dir,\n",
    "            causal=True,\n",
    "            bundle_input_init_states_fn = False \n",
    "        )\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "        # Optimization configuration\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        # FP16 conversion\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS     # Enable select TensorFlow ops\n",
    "        ]\n",
    "        converter.experimental_new_converter = True\n",
    "\n",
    "        try:\n",
    "            tflite_model = converter.convert()\n",
    "            print(f\"Successfully converted {model_id} to TFLite\")\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            # Save with comprehensive filename\n",
    "            fp16_filename = (\n",
    "                f\"movinet_{model_id}_with_\"\n",
    "                f\"_train-size_{100 - split} and test-size_{split}_\"\n",
    "                f\"test accuracy{accuracy*100:.2f}_\"\n",
    "                f\"batch {batch_type}_\"\n",
    "                f\"number of frames at a time {num_frames}_\"\n",
    "                f\"at {timestamp}_ fp16_with_frame resolution and bundle_input_init_states_fn was set to False\"\n",
    "                f\"{resolution}.tflite\"\n",
    "            )\n",
    "            \n",
    "            with open(fp16_filename, 'wb') as f:\n",
    "                f.write(tflite_model)\n",
    "                print(f\"Saved TFLite model to {fp16_filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Conversion failed for {model_id}: {str(e)}\")\n",
    "            continue\n",
    "  \n",
    "        exported_model_size = os.path.getsize(fp16_filename)\n",
    "        \n",
    "        exported_models[f'fp16_{batch_type}batch_{num_frames}frames'] = {\n",
    "            'filename': fp16_filename,\n",
    "            'input_shape': input_shape,\n",
    "            'initial_model_size': initial_model_size,\n",
    "            'exported_model_size': exported_model_size,\n",
    "            'initial_num_ops': initial_num_ops,\n",
    "            'exported_num_ops': 'fp16',\n",
    "            'total_parameters': initial_params\n",
    "        }\n",
    "\n",
    "    # Save information to JSON file\n",
    "    json_filename = f'model_{model_id}_export_info.json'\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(exported_models, f, indent=4)\n",
    "\n",
    "    return exported_models\n",
    "\n",
    "def get_model_size(model_weights):\n",
    "    \n",
    "    size = os.path.getsize(model_weights)\n",
    "    return size\n",
    "\n",
    "def get_numerical_operations(model):\n",
    "    # Determine the numerical operations used in the model\n",
    "    if tf.keras.mixed_precision.global_policy().name == 'mixed_float16':\n",
    "        return 'mixed_float16'\n",
    "    else:\n",
    "        return 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc25fbbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T11:57:23.545848Z",
     "iopub.status.busy": "2025-03-03T11:57:23.545556Z",
     "iopub.status.idle": "2025-03-03T16:34:08.701562Z",
     "shell.execute_reply": "2025-03-03T16:34:08.700642Z"
    },
    "papermill": {
     "duration": 16605.628166,
     "end_time": "2025-03-03T16:34:09.147913",
     "exception": false,
     "start_time": "2025-03-03T11:57:23.519747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MoViNet-A3 with resolution 256\n",
      "\n",
      "Plot saved: train_test_split_A3_split_70_30.png\n",
      "Shape: (8, 8, 256, 256, 3)\n",
      "Label: (8,)\n",
      "\n",
      "Loaded weights for MoViNet-A3 from movinet_a3_stream/ckpt-1\n",
      "Epoch 1/30\n",
      "    239/Unknown - 536s 2s/step - loss: 1.0961 - accuracy: 0.6145\n",
      "Epoch 1: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 700s 2s/step - loss: 1.0961 - accuracy: 0.6145 - val_loss: 1.7346 - val_accuracy: 0.6546 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.5550 - accuracy: 0.8117\n",
      "Epoch 2: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 536s 2s/step - loss: 0.5550 - accuracy: 0.8117 - val_loss: 0.6449 - val_accuracy: 0.8056 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.3107 - accuracy: 0.9012\n",
      "Epoch 3: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 536s 2s/step - loss: 0.3107 - accuracy: 0.9012 - val_loss: 0.6157 - val_accuracy: 0.8333 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.9252\n",
      "Epoch 4: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 536s 2s/step - loss: 0.2418 - accuracy: 0.9252 - val_loss: 0.9926 - val_accuracy: 0.7403 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.1805 - accuracy: 0.9419\n",
      "Epoch 5: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 535s 2s/step - loss: 0.1805 - accuracy: 0.9419 - val_loss: 0.2937 - val_accuracy: 0.9022 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9529\n",
      "Epoch 6: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 520s 2s/step - loss: 0.1375 - accuracy: 0.9529 - val_loss: 0.6242 - val_accuracy: 0.8188 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9446\n",
      "Epoch 7: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 507s 2s/step - loss: 0.1669 - accuracy: 0.9446 - val_loss: 0.4204 - val_accuracy: 0.8829 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9613\n",
      "Epoch 8: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 512s 2s/step - loss: 0.1154 - accuracy: 0.9613 - val_loss: 0.4706 - val_accuracy: 0.8756 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9712\n",
      "Epoch 9: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 505s 2s/step - loss: 0.0855 - accuracy: 0.9712 - val_loss: 0.3645 - val_accuracy: 0.9251 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9780\n",
      "Epoch 10: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 508s 2s/step - loss: 0.0795 - accuracy: 0.9780 - val_loss: 0.2369 - val_accuracy: 0.9336 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9817\n",
      "Epoch 11: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 509s 2s/step - loss: 0.0671 - accuracy: 0.9817 - val_loss: 0.2101 - val_accuracy: 0.9481 - lr: 9.0484e-04 - learning_rate: 9.0484e-04\n",
      "Epoch 12/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9937\n",
      "Epoch 12: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 511s 2s/step - loss: 0.0265 - accuracy: 0.9937 - val_loss: 0.2462 - val_accuracy: 0.9336 - lr: 8.1873e-04 - learning_rate: 8.1873e-04\n",
      "Epoch 13/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9718\n",
      "Epoch 13: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 504s 2s/step - loss: 0.0790 - accuracy: 0.9718 - val_loss: 0.3109 - val_accuracy: 0.9227 - lr: 7.4082e-04 - learning_rate: 7.4082e-04\n",
      "Epoch 14/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9911\n",
      "Epoch 14: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 507s 2s/step - loss: 0.0258 - accuracy: 0.9911 - val_loss: 0.1667 - val_accuracy: 0.9589 - lr: 6.7032e-04 - learning_rate: 6.7032e-04\n",
      "Epoch 15/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9932\n",
      "Epoch 15: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 504s 2s/step - loss: 0.0298 - accuracy: 0.9932 - val_loss: 0.1310 - val_accuracy: 0.9674 - lr: 6.0653e-04 - learning_rate: 6.0653e-04\n",
      "Epoch 16/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9974\n",
      "Epoch 16: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 505s 2s/step - loss: 0.0115 - accuracy: 0.9974 - val_loss: 0.1416 - val_accuracy: 0.9650 - lr: 5.4881e-04 - learning_rate: 5.4881e-04\n",
      "Epoch 17/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9969\n",
      "Epoch 17: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 504s 2s/step - loss: 0.0071 - accuracy: 0.9969 - val_loss: 0.1608 - val_accuracy: 0.9710 - lr: 4.9659e-04 - learning_rate: 4.9659e-04\n",
      "Epoch 18/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 18: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 510s 2s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1461 - val_accuracy: 0.9710 - lr: 4.4933e-04 - learning_rate: 4.4933e-04\n",
      "Epoch 19/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9995\n",
      "Epoch 19: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 507s 2s/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.1451 - val_accuracy: 0.9734 - lr: 4.0657e-04 - learning_rate: 4.0657e-04\n",
      "Epoch 20/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 5.1377e-04 - accuracy: 1.0000\n",
      "Epoch 20: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 509s 2s/step - loss: 5.1377e-04 - accuracy: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.9722 - lr: 3.6788e-04 - learning_rate: 3.6788e-04\n",
      "Epoch 21/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9990\n",
      "Epoch 21: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 508s 2s/step - loss: 0.0026 - accuracy: 0.9990 - val_loss: 0.1311 - val_accuracy: 0.9771 - lr: 3.3287e-04 - learning_rate: 3.3287e-04\n",
      "Epoch 22/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9984\n",
      "Epoch 22: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 511s 2s/step - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.1202 - val_accuracy: 0.9734 - lr: 3.0119e-04 - learning_rate: 3.0119e-04\n",
      "Epoch 23/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9979\n",
      "Epoch 23: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 506s 2s/step - loss: 0.0100 - accuracy: 0.9979 - val_loss: 0.1397 - val_accuracy: 0.9686 - lr: 2.7253e-04 - learning_rate: 2.7253e-04\n",
      "Epoch 24/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9984\n",
      "Epoch 24: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 500s 2s/step - loss: 0.0111 - accuracy: 0.9984 - val_loss: 0.1375 - val_accuracy: 0.9686 - lr: 2.4660e-04 - learning_rate: 2.4660e-04\n",
      "Epoch 25/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 7.8526e-04 - accuracy: 1.0000\n",
      "Epoch 25: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 496s 2s/step - loss: 7.8526e-04 - accuracy: 1.0000 - val_loss: 0.1365 - val_accuracy: 0.9746 - lr: 2.2313e-04 - learning_rate: 2.2313e-04\n",
      "Epoch 26/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 5.3020e-04 - accuracy: 1.0000\n",
      "Epoch 26: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 503s 2s/step - loss: 5.3020e-04 - accuracy: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.9710 - lr: 2.0190e-04 - learning_rate: 2.0190e-04\n",
      "Epoch 27/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 2.1694e-04 - accuracy: 1.0000\n",
      "Epoch 27: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 499s 2s/step - loss: 2.1694e-04 - accuracy: 1.0000 - val_loss: 0.1441 - val_accuracy: 0.9746 - lr: 1.8268e-04 - learning_rate: 1.8268e-04\n",
      "Epoch 28/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 8.2131e-04 - accuracy: 1.0000\n",
      "Epoch 28: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 499s 2s/step - loss: 8.2131e-04 - accuracy: 1.0000 - val_loss: 0.1396 - val_accuracy: 0.9734 - lr: 1.6530e-04 - learning_rate: 1.6530e-04\n",
      "Epoch 29/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 3.9990e-04 - accuracy: 1.0000\n",
      "Epoch 29: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 496s 2s/step - loss: 3.9990e-04 - accuracy: 1.0000 - val_loss: 0.1305 - val_accuracy: 0.9758 - lr: 1.4957e-04 - learning_rate: 1.4957e-04\n",
      "Epoch 30/30\n",
      "239/239 [==============================] - ETA: 0s - loss: 3.0379e-04 - accuracy: 1.0000\n",
      "Epoch 30: saving model to trained_model.weights.h5\n",
      "239/239 [==============================] - 497s 2s/step - loss: 3.0379e-04 - accuracy: 1.0000 - val_loss: 0.1392 - val_accuracy: 0.9710 - lr: 1.3534e-04 - learning_rate: 1.3534e-04\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "104/104 [==============================] - 120s 1s/step - loss: 0.1189 - accuracy: 0.9746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-b777c728350a>:23: MatplotlibDeprecationWarning:\n",
      "\n",
      "The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 17s 17s/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 289ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 289ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 298ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 318ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 304ms/step\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "Enhanced confusion matrix saved as confusion_matrix_A3_test_accuracy_97.4638.png\n",
      "Classification report saved as classification_report_A3_test_at_training_split_70.0_30.0.csv\n",
      "\n",
      "Accuracy: 0.9746\n",
      "Successfully converted a3 to TFLite\n",
      "Saved TFLite model to movinet_a3_with__train-size_70.0 and test-size_30.0_test accuracy97.46_batch single_number of frames at a time 8_at 20250303-162737_ fp16_with_frame resolution and bundle_input_init_states_fn was set to False256.tflite\n",
      "Successfully converted a3 to TFLite\n",
      "Saved TFLite model to movinet_a3_with__train-size_70.0 and test-size_30.0_test accuracy97.46_batch dual_number of frames at a time 8_at 20250303-163408_ fp16_with_frame resolution and bundle_input_init_states_fn was set to False256.tflite\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def setup_and_train_movinet_models(models_dict):\n",
    "    for model_id, params in models_dict.items():\n",
    "        batch_size = params['batch_size']\n",
    "        num_frames = params['num_frames']\n",
    "        resolution = params['resolution']\n",
    "        split = params['split']\n",
    "\n",
    "        print(f\"Setting up MoViNet-{model_id} with resolution {resolution}\\n\")\n",
    "        \n",
    "        train_dict, test_dict = train_test_split(t, test_split=split)\n",
    "        per_model(t, train_dict, test_dict, model_id, split)\n",
    "        \n",
    "        train_generator = FrameGenerator(train_dict, num_frames, resolution, training=True)\n",
    "        test_generator = FrameGenerator(test_dict, num_frames, resolution, training=False)\n",
    "\n",
    "        train_ds = prepare_dataset(train_generator, batch_size)\n",
    "        test_ds = prepare_dataset(test_generator, batch_size)\n",
    "\n",
    "        for frames, labels in train_ds.take(1):\n",
    "              print(f\"Shape: {frames.shape}\")\n",
    "              print(f\"Label: {labels.shape}\")\n",
    "         \n",
    "        \n",
    "        use_positional_encoding = model_id in {'A3', 'A4', 'A5'}\n",
    "\n",
    "        backbone = movinet.Movinet(\n",
    "            model_id=model_id.lower(),\n",
    "            causal=True,\n",
    "            conv_type='2plus1d',\n",
    "            se_type='2plus3d',\n",
    "            activation='hard_swish',\n",
    "            gating_activation='hard_sigmoid',\n",
    "            use_positional_encoding=use_positional_encoding,\n",
    "            use_external_states=False\n",
    "        )\n",
    "\n",
    "        backbone.Trainable = False\n",
    "\n",
    "        model = movinet_model.MovinetClassifier(\n",
    "            backbone,\n",
    "            num_classes=600,\n",
    "            output_states=True\n",
    "        )\n",
    "\n",
    "        # Load pre-trained weights\n",
    "        url = f\"https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_{model_id.lower()}_stream.tar.gz\"\n",
    "        tar_filename = f\"movinet_{model_id.lower()}_stream.tar.gz\"\n",
    "        checkpoint_dir = f\"movinet_{model_id.lower()}_stream\"\n",
    "\n",
    "        os.system(f\"wget {url} -O {tar_filename} -q\")\n",
    "        os.system(f\"tar -xvf {tar_filename}\")\n",
    "\n",
    "        checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        checkpoint = tf.train.Checkpoint(model=model)\n",
    "        status = checkpoint.restore(checkpoint_path)\n",
    "        status.assert_existing_objects_matched()\n",
    "        print(f\"\\nLoaded weights for MoViNet-{model_id} from {checkpoint_path}\")\n",
    "\n",
    "        model = build_classifier(batch_size, num_frames, resolution, backbone, 8)\n",
    "        \n",
    "        loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(loss=loss_obj, optimizer='adam', metrics=['accuracy'])\n",
    "        lr_scheduler = CustomLearningRateScheduler(lr_schedule)\n",
    "\n",
    "        callbacks = [\n",
    "            CustomLearningRateLogger(),\n",
    "            CustomModelCheckpoint(\n",
    "                filepath=\"trained_model.weights.h5\",\n",
    "                save_weights_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            CustomReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=10,\n",
    "                min_lr=1e-10,\n",
    "                verbose=1,\n",
    "                cooldown=1,\n",
    "                min_delta=1e-3\n",
    "            ),\n",
    "            CustomEarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            lr_scheduler,\n",
    "            CustomTensorBoard(log_dir='./logs')\n",
    "        ]\n",
    "\n",
    "        results = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=test_ds,\n",
    "            epochs=30,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "            # class_weight=class_weights\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluation_results = model.evaluate(test_ds,return_dict=True)\n",
    "        accuracy = evaluation_results['accuracy']\n",
    "    \n",
    "\n",
    "        # Plot training metrics\n",
    "        plot_training_metrics(results, model_id, 0.001, batch_size, 'adam', resolution, split, accuracy)\n",
    "       \n",
    "        # Get actual and predicted labels\n",
    "        actual, predicted = get_actual_predicted_labels(test_ds, model)\n",
    "        label_names = list(test_generator.class_ids_for_name.keys())\n",
    "                \n",
    "        plot_confusion_matrix(actual, predicted, label_names, 'test', model_id,accuracy,split)\n",
    "        plot_confusion_matrix2(actual, predicted, label_names, 'test', model_id,accuracy,split)\n",
    "    \n",
    "        print(f'\\nAccuracy: {accuracy:.4f}')     \n",
    "        \n",
    "        model_id = model_id.lower()\n",
    "        use_positional_encoding = model_id in {'a3', 'a4', 'a5'}\n",
    "        resolution = resolution\n",
    "\n",
    "        # Create backbone and model.\n",
    "        backbone = movinet.Movinet(\n",
    "            model_id=model_id,\n",
    "            causal=True,\n",
    "            conv_type='2plus1d',\n",
    "            se_type='2plus3d',\n",
    "            activation='hard_swish',\n",
    "            gating_activation='hard_sigmoid',\n",
    "            use_positional_encoding=use_positional_encoding,\n",
    "            use_external_states=True,\n",
    "        )\n",
    "\n",
    "        model = movinet_model.MovinetClassifier(\n",
    "                backbone,\n",
    "                num_classes=8,\n",
    "                output_states=True)\n",
    "\n",
    "        model.load_weights('/kaggle/working/trained_model.weights.h5')\n",
    "        # Export model variants\n",
    "        exported_models = export_model_variants(model, model_id, resolution, num_frames, accuracy, split, train_dict, '/kaggle/working/trained_model.weights.h5')\n",
    "        # export_model_variants(model, model_id, resolution, 1, accuracy, split, train_dict, '/kaggle/working/trained_model.weights.h5')\n",
    "setup_and_train_movinet_models(models)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4854603,
     "sourceId": 8195915,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16795.305871,
   "end_time": "2025-03-03T16:34:14.370643",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-03T11:54:19.064772",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
